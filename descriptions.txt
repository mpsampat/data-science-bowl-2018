Jan 17
First attempt: LB 0.247

Nuclei Overview to submission: LB 0.149
------------------------------------------
Jan 19, same kernel, LB 0.248
Mean iou for train as well as test is 81%
------------------------------------------
Jan 20, same kernel
Mean iou:  88, new iou: ~45., lower LB scores
High accuracy is easy, but that is easy and does not mean much
Mean iou is high enough, but new iou (in which I have more confidence
is not high). So, I believe we can fit same data better.
------------------------------------------
Jan 21
With keras augmentations, new iou 43%, LB 0.269
loss: 0.0838 - acc: 0.9700 - mean_iou: 0.8273 - new_iou: 0.4244 - 
val_loss: 0.0652 - val_acc: 0.9752 - val_mean_iou: 0.8275 - val_new_iou: 0.4344
-------------------------------------------
Jan 22
With R's unet (unet-8)
loss: 0.0671 - acc: 0.9737 - mean_iou: 0.8517 - new_iou: 0.4315 - val_loss: 0.0728 - val_acc: 0.9727 - val_mean_iou: 0.8518 - val_new_iou: 0.4312
LB 0.301
------------------------
Jan 26-27
With unet-5:
Let's check num rles on train:
Total: 29K, predicted 20K
Averge deviation: 33.2%

Then I trained with 256x256 crops
Average deviation: 35%

Then even while prediction I do crops
Average deviation: 32.8%

Cropping while prediction is not good. Cropping while training is a good idea.

We need to make cropping work while predicting, since many images are larger. Question is: how to make it work.
With cropping, problem is that sometimes too many nuclei are predicted.
So, the main problem is finding the number of nuclei.
-----------------------
Now, the main ideas are: 
- get same number of training examples from each microscope, so as not to overfit.
- augmentation with random crops
-----------------------
Jan 28
- With 7 augmentations and many cycles, got 30.6% which is higehst ever.
  You should continue to train as long as mean iou keeps increasing.
- Now let us try different sized random crops
